# Default values for product-recommender-system.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# This will set the replicaset count more information can be found here: https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/
replicaCount: 1



# This is for the secrets for pulling an image from a private repository more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
imagePullSecrets: []
# This is to override the chart name.
nameOverride: ""
fullnameOverride: ""

# This section builds out the service account more information can be found here: https://kubernetes.io/docs/concepts/security/service-accounts/
serviceAccount:
  # Specifies whether a service account should be created
  create: false
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

# This is for setting Kubernetes Annotations to a Pod.
# For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
podAnnotations: {}
# This is for setting Kubernetes Labels to a Pod.
# For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
podLabels: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

# This is for setting up a service more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/
service:
  # This sets the service type more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types
  type: ClusterIP
  # This sets the ports more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#field-spec-ports
  port: 80
  targetPort: 8080

# Backend specific configuration
backend:
  # Service configuration for the backend
  service:
    type: ClusterIP
    port: 8000
    targetPort: 8000

  # Pod restart configuration
  restartPolicy: Always
  backoffLimit: 12

  # Health check configuration
  healthCheck:
    livenessProbe:
      path: /health
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 5
    readinessProbe:
      path: /health
      initialDelaySeconds: 15
      periodSeconds: 10
      timeoutSeconds: 5

  # Resource configuration for the backend
  resources:
    limits:
      cpu: "2"
      memory: "2Gi"
    requests:
      cpu: "500m"
      memory: "1Gi"

  # Additional environment variables specific to backend
  additionalEnv:
    - name: DATABASE_URL
      valueFrom:
        secretKeyRef:
          name: pgvector
          key: uri
    - name: LLM_API_KEY
      valueFrom:
        secretKeyRef:
          name: llama-api
          key: LLM_API_KEY
  # - name: EXAMPLE_VAR
  #   value: "example-value"

  # Additional volume mounts specific to backend
  additionalVolumeMounts: []
  # - name: config-volume
  #   mountPath: /etc/config

  # Additional volumes specific to backend
  additionalVolumes: []
  # - name: config-volume
  #   configMap:
  #     name: backend-config
  pullPolicy: Always

# Special configuration for the LLM API key
llm:
  secret:
    create: true
    name: llama-api
    data:
      LLM_API_KEY: ""
resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

# OpenShift Route configuration
route:
  enabled: true
  # host: your-app.apps.your-cluster.com  # Optional: specify custom hostname
  tls:
    enabled: true  # Enable HTTPS

# This is to setup the liveness and readiness probes more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
livenessProbe:
  httpGet:
    path: /
    port: http
readinessProbe:
  httpGet:
    path: /
    port: http

# This section is for setting up autoscaling more information can be found here: https://kubernetes.io/docs/concepts/workloads/autoscaling/
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

# Additional volumes on the output Deployment definition.
volumes: []
# - name: foo
#   secret:
#     secretName: mysecret
#     optional: false

# Additional volumeMounts on the output Deployment definition.
volumeMounts: []
# - name: foo
#   mountPath: "/etc/foo"
#   readOnly: true

nodeSelector: {}

tolerations: {}

affinity: {}

pipelineJobImage: quay.io/rh-ai-quickstart/recommendation-training:latest
applicationImage: quay.io/rh-ai-quickstart/recommendation-core:latest
# Note the backend uses recommendation_core as library.
frontendBackendImage: quay.io/rh-ai-quickstart/product-recommender-frontend-backend:latest

dbName: product_recommender_system

env:
  - name: DB_HOST
    valueFrom:
      secretKeyRef:
        name: pgvector
        key: host
  - name: DB_PORT
    valueFrom:
      secretKeyRef:
        name: pgvector
        key: port
  - name: DB_USER
    valueFrom:
      secretKeyRef:
        name: pgvector
        key: user
  - name: DB_PASSWORD
    valueFrom:
      secretKeyRef:
        name: pgvector
        key: password
  - name: DB_NAME
    valueFrom:
      secretKeyRef:
        name: pgvector
        key: dbname
  - name: USE_LLM_FOR_REVIEWS
    value: "true"
  - name: LLM_API_BASE
    value: "https://llama-3-3-70b-instruct-w8a8-llama-3-70b-quantized.apps.ai-dev04.kni.syseng.devcluster.openshift.com/v1"
  - name: LLM_MODEL
    value: "llama-3-3-70b-instruct-w8a8"
  - name: LLM_TIMEOUT
    value: "60"
  - name: OLLAMA_MODEL
    value: "meta-llama/Llama-3.1-8B-Instruct"
  - name: MODEL_ENDPOINT
    value: "http://llama-3-1-8b-instruct-predictor-kickstart-llms.apps.ai-dev02.kni.syseng.devcluster.openshift.com/v1"

minio:
  env:
    - name: MINIO_HOST
      valueFrom:
        secretKeyRef:
          name: ds-pipeline-s3-dspa
          key: host
    - name: MINIO_PORT
      valueFrom:
        secretKeyRef:
          name: ds-pipeline-s3-dspa
          key: port
    - name: MINIO_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: ds-pipeline-s3-dspa
          key: accesskey
    - name: MINIO_SECRET_KEY
      valueFrom:
        secretKeyRef:
          name: ds-pipeline-s3-dspa
          key: secretkey

feast:
  project: feast_rec_sys
  secret: feast-feast-recommendation-registry-tls
  registry: feast-feast-recommendation-registry

configure-pipeline:
  notebook:
    create: false

strimzi-kafka-operator:
  extraEnvs:
    - name: STRIMZI_USE_FINALIZERS
      value: "false"

# End of configuration
